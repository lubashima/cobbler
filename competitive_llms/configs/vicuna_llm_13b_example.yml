framework: "huggingface"


device: "cuda"
mode: "AutoModelForCausalLM"
model:
  name: "/corpora/models/Vicuna/13B" # Fill out the model path e.g. /home/jonginn/volume/models/LLaMA/7B
  device_map: "auto"

batch_size: 20
skip_special_tokens: true

# Parameters for GenerationConfig
# Please do check the documentation: c.f. https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig
params:
  max_new_tokens: 128
  early_stopping: true
  num_beams: 3
  do_sample: false
  temperature: 0.0
  top_p: 1.0
  top_k: 50
  do_sample: true
  num_return_sequences: 1
